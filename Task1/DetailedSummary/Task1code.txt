%pip install
%pip install --upgrade pip
%pip install pandas numpy matplotlib seaborn
import pandas as pd
import numpy as np

#Step: 2

train_data = pd.read_csv('D:/internship/dataset/train.csv')
test_data = pd.read_csv('D:/internship/dataset/test.csv')
train_data.head()
train_data.isnull().sum()

#step: 3

# Handle missing values
train_data = train_data.assign(
    Age=train_data['Age'].fillna(train_data['Age'].median()),
    Fare=train_data['Fare'].fillna(train_data['Fare'].median())
)
train_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])
test_data['Age'] = test_data['Age'].fillna(test_data['Age'].median())
test_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())
test_data['Embarked'] = test_data['Embarked'].fillna(test_data['Embarked'].mode()[0])

#Step: 4

# Remove duplicate records if any
train_data.drop_duplicates(inplace=True)
test_data.drop_duplicates(inplace=True)

#Step: 5

# Check data types
train_data.dtypes

# Convert data types if necessary
# Example: If 'Fare' was read as an object, we would convert it to float
# train_data['Fare'] = train_data['Fare'].astype(float)

#Step:6

# Summary statistics for numerical features
train_data.describe()

# Summary statistics for categorical features
train_data.describe(include=['O'])

import matplotlib.pyplot as plt
import seaborn as sns

#Step:7

train_data.hist(bins=30, figsize=(20, 15))
plt.show()

#Step: 7

plt.figure(figsize=(20, 15))
for i, column in enumerate(['Age', 'Fare']):
    plt.subplot(2, 1, i+1)
    sns.boxplot(data=train_data[column])
plt.show()

#Step:8

# Scatter plots
plt.figure(figsize=(20, 15))
sns.scatterplot(data=train_data, x='Age', y='Fare', hue='Survived')
plt.show()

#step:9

# Select only numeric columns
numeric_data = train_data.select_dtypes(include=[np.number])

# Compute the correlation matrix
corr_matrix = numeric_data.corr()

# Visualize the correlation matrix
plt.figure(figsize=(15, 10))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.show()

#step 10 updated from error

# Example: Create a new feature 'FamilySize'
train_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1
test_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1

# Example: Create a new feature 'IsAlone'
train_data['IsAlone'] = (train_data['FamilySize'] == 1).astype(int)
test_data['IsAlone'] = (test_data['FamilySize'] == 1).astype(int)

# Step 11: Updated from error
# Check if columns exist before one-hot encoding
columns_to_encode = ['Sex', 'Embarked']

for col in columns_to_encode:
    if col in train_data.columns and col in test_data.columns:
        train_data = pd.get_dummies(train_data, columns=[col])
        test_data = pd.get_dummies(test_data, columns=[col])

# Ensure both train and test data have the same columns after encoding
train_data, test_data = train_data.align(test_data, join='left', axis=1, fill_value=0)

!pip install scikit-learn

%pip install scikit-learn

from sklearn.preprocessing import StandardScaler

# Step 12: (Updated) 
#Standardize numerical features
scaler = StandardScaler()
numerical_features = ['Age', 'Fare', 'FamilySize']

train_data[numerical_features] = scaler.fit_transform(train_data[numerical_features])
test_data[numerical_features] = scaler.transform(test_data[numerical_features])

# Check the result
print(train_data.head())
print(test_data.head())

#Step: 13
from sklearn.utils import resample

#Step: 13
# Check for imbalance in the target variable
train_data['Survived'].value_counts()

# If imbalance is found, use resampling techniques
# Example: Upsample minority class
survived = train_data[train_data['Survived'] == 1]
not_survived = train_data[train_data['Survived'] == 0]

survived_upsampled = resample(survived, replace=True, n_samples=len(not_survived), random_state=42)
train_data = pd.concat([not_survived, survived_upsampled])

# Check the result
print(train_data.head())
print(test_data.head())
































